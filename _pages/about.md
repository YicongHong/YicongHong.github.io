---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style type="text/css">
#touch {
 background-color: #bbb;
 padding: .4em;
 -moz-border-radius: 5px;
 -webkit-border-radius: 5px;
 border-radius: 6px;
 color: #fff;
 font-size: 14px;
 text-decoration: none;
 border: none;
}
#touch:hover {
 border: none;
 background: orange;
 box-shadow: 0px 0px 1px #777;
}
</style>

<style type="text/css">
a:link {text-decoration: none; }
a:hover { text-decoration: underline; }
</style>

------
I am a research scientist at <a target="_blank" href="https://research.adobe.com/">Adobe Research<a/>. My current focus is video generation.

Prior to that, I completed my Ph.D. (Dec2023) at the <a target="_blank" href="https://cecc.anu.edu.au/">College of Engineering, Computing, and Cybernetics (CECC)<a/> at the <a target="_blank" href="https://www.anu.edu.au/">Australian National University (ANU)<a/>. I was also a previous research student at the <a target="_blank" href="https://www.roboticvision.org/">Australian Centre for Robotic Vision (ACRV@ANU)<a/>. I was advised by <a target="_blank" href="http://users.cecs.anu.edu.au/~sgould/">Prof. Stephen Gould<a/> (ANU), <a target="_blank" href="http://www.qi-wu.me/">Prof. Qi Wu<a/> (UoA) and <a target="_blank" href="http://users.cecs.anu.edu.au/~xlx/">Prof. Lexing Xie<a/> (ANU).

<!--Prior to that, in Nov'2018, I received my bachelor's degree of engineering in mechatronic systems with first-class honours in the College of Engineering and Computer Science at ANU. In 2018, I was also a part-time research student at the <a target="_blank" href="https://data61.csiro.au/">Data61, CSIRO<a/>, working on human pose and shape visualization.-->

<!--I have a broad research interest in computer vision, natural language processing, and robotics. -->

I love Embodied AI and AICG! -- "*What I cannot create I do not understand.*"

My latest works include **Video/3D Generation** and **Training LLMs/VLMs for Navigation**.

------

News
======
**2024.02.12** &emsp; Finally! My first day in **Adobe Research** as a full-time research scientist! So happy to be back and working with everyone! ğŸ˜ŠğŸ”¥ğŸ”¥

**2024.01.20** &emsp; Thrilled to share that our **LRM: Large Reconstruction Model for Single Image to 3D** <a target="_blank" href="https://yiconghong.me/LRM/"><button id="touch">Project Page</button></a> <a target="_blank" href="https://arxiv.org/abs/2311.04400"><button id="touch">PDF</button></a> and **Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model** <a target="_blank" href="https://jiahao.ai/instant3d/"><button id="touch">Project Page</button></a> <a target="_blank" href="https://arxiv.org/abs/2311.06214"><button id="touch">PDF</button></a> have been accepted to ICLR2024 as Oral and Poster papers! ğŸ˜†ğŸ‰ğŸ‰
A wonderful ending to my PhD <a target="_blank" href="https://cecc.anu.edu.au/">@ANUCECC<a/> and a fantastic start to my new journey <a target="_blank" href="https://research.adobe.com/">@AdobeResearch<a/>! Thanks <a target="_blank" href="https://research.adobe.com/person/hao-tan/">@HaoTan<a/> for your recognition and your great adviseâ­! Thanks Team ğŸ™Œ! Thanks Adobe! ğŸ˜Šâ¤ï¸â¤ï¸

**2023.12.09** &emsp; Congrats to <a target="_blank" href="https://www.linkedin.com/in/gengze-zhou-159095203/?locale=en_US">@GengzeZhou<a/> for his paper acceptance to AAAI2024 -- **NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models**! An amazing work at the start of his PhD and a wonderful attempt to integrate LLM and Embodied Agents! ğŸ˜€ğŸ”¥ğŸ”¥ <a target="_blank" href="https://arxiv.org/abs/2305.16986"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/GengzeZhou/NavGPT"><button id="touch">Code&Data</button></a>

**2023.07.14** &emsp; Our papers **Scaling Data Generation in Vision-and-Language Navigation** (Oral) <a target="_blank" href="https://arxiv.org/abs/2307.15644"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/wz0919/ScaleVLN"><button id="touch">Code</button></a> and **Learning Navigational Visual Representations with Semantic Map Supervision** <a target="_blank" href="https://arxiv.org/abs/2307.12335"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Ego2Map-NaViT"><button id="touch">Code</button></a> have been accepted to ICCV 2023! The projects were completed/initialized during my first internship at Adobe! It was my great pleasure to work on them with my friends around the world (<a target="_blank" href="https://scholar.google.com/citations?user=G-jPT9MAAAAJ&hl=en">@ZunWang<a/>, <a target="_blank" href="https://jialuli-luka.github.io/">@JialuLi<a/>, <a target="_blank" href="https://research.adobe.com/person/hao-tan/">@HaoTan<a/>)! ğŸ˜€ğŸ˜Šâ¤ï¸ Thank heaps <a target="_blank" href="https://opengvlab.shlab.org.cn">OpenGVLab@Shanghai AI Laboratory<a/> for the great support! â­ğŸ™Œ

**2023.02.19** &emsp; Join **Adobe Research** again (intern)! Working on Text-to-3D Generation and Single-Image-to-3D Reconstruction, totally unfamiliar topics to me! ğŸ˜ŠğŸ”¥ğŸ”¥
  
**2022.12.29** &emsp; Paper **HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation** by Yanyuan Qiao, Yuankai Qi, Zheng Yu, Peng Wang, Qi Wu and myself has been accepted by TPAMI! Congrats Yanyuan!!! ğŸ˜€ğŸ˜€ğŸ˜€ <a target="_blank" href="https://ieeexplore.ieee.org/document/10006384"><button id="touch">PDF</button></a>
  
**2022.06.19**
- Attending CVPR2022 in person!!! Finally meeting so many great researchers! I have learned so much!!! â¤ï¸â¤ï¸â¤ï¸
- Congrats to Zun Wang, Dong An and Team **JoyBoy** for winning the **1st Place in the Room-Across-Room (RxR) Habitat Challenge 2022**!!! ğŸ˜†âš¡âš¡ <a target="_blank" href="https://arxiv.org/abs/2206.11610"><button id="touch">Report</button></a> <a target="_blank" href="https://drive.google.com/file/d/15VbXcanw7D3q5TUm75WmDVslqgOmVvJk/view?usp=sharing"><button id="touch">Certificate</button></a>

**2022.06.15** &emsp; Visiting Professor Eric Xin Wang and the **ERIC Lab at the University of California Santa Cruz**! It was amazing to learn from so many young researchers! ğŸ˜„
  
**2022.05.10** &emsp; Invited talk by the **NLP Lab at the Fudan University**, really enjoyed chatting with everyone! ğŸ˜„

**2022.03.28** &emsp; My VLN project has been selected to be a part of the **NVIDIA Academic Hardware Grant Program**! ğŸ˜† Thank you so much NVIDIA for the A100 GPU grant!!! ğŸ˜­ğŸ˜­ğŸ˜­
  
**2022.03.14** &emsp; I have started a research internship at the **Creative Intelligence Lab in Adobe Research** in San Jose, California, US!!! ğŸ˜†ğŸ˜†ğŸ˜†

**2022.03.02**
- Our paper **Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation** has been accepted to CVPR 2022! ğŸ˜Š I am so happy to share lots of thoughts about VLN in this paper! See you guys in New Orleans! â¤ï¸ <a target="_blank" href="https://arxiv.org/abs/2203.02764"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Discrete-Continuous-VLN"><button id="touch">Code</button></a>
- Paper **HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation** by Yanyuan Qiao, Yuankai Qi, Peng Wang, Qi Wu and myself has been accepted to CVPR 2022! Congrats Yanyuan on the first paper in her PhD! ğŸ˜€ <a target="_blank" href="https://arxiv.org/abs/2203.11591"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YanyuanQiao/HOP-VLN"><button id="touch">Code</button></a>
  
**2021.08.17** &emsp; Paper **The Road To Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation** by Yuankai Qi, Zizheng Pan, Ming-Hsuan Yang, Anton van den Hengel, Qi Wu and myself has been accepted to ICCV 2021! ğŸ˜€ <a target="_blank" href="https://arxiv.org/abs/2104.04167"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YuankaiQi/ORIST"><button id="touch">Code</button></a>

**2021.04.10** &emsp; Paper **Learning Structure-Aware Semantic Segmentation with Image-Level Supervision** by Jiawei Liu, Dr. Jing Zhang, Prof. Nick Barnes and myself, has been accepted to IJCNN 2021! Congrats Jiawei on his first paper in computer vision! ğŸ˜€ <a target="_blank" href="https://arxiv.org/abs/2104.07216"><button id="touch">PDF</button></a>

**2021.03.16** &emsp; Our <a target="_blank" href="https://github.com/YicongHong/Thinking-VLN">**Thinking-VLN**</a> repo is online! Come to enjoy our immature ideas and share your thoughts! Just for FUN thinking!

**2021.03.06** &emsp; Our paper **A Recurrent Vision-and-Language BERT for Navigation** has been accepted to CVPR 2021 as an Oral paper with 3 strong accepts! ğŸ˜†ğŸ˜†ğŸ˜† <a target="_blank" href="https://arxiv.org/abs/2011.13922"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Recurrent-VLN-BERT"><button id="touch">Code</button></a>

**2020.10.05** &emsp; I gave a guest lecture in the Deep Learning Course at ANU (ENGN8536) about Vision and Language Research! My first lecture at Uni! Nervous and Fun! ğŸ˜€ <a target="_blank" href="https://drive.google.com/file/d/1Rsy8gFK0seWVgDJ6Uc0UU9MXO9F23EKY/view?usp=sharing"><button id="touch">PDF</button></a>

**2020.09.26** &emsp; Our paper **Language and Visual Entity Relationship Graph for Agent Navigation** has been accepted to NeurIPS 2020! ğŸ˜€ <a target="_blank" href="https://arxiv.org/abs/2010.09304"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Entity-Graph-VLN"><button id="touch">Code</button></a>

**2020.09.15** &emsp; Our paper **Sub-Instruction Aware Vision-and-Language Navigation** has been accepted to EMNLP 2020! My first paper! ğŸ˜Š <a target="_blank" href="https://arxiv.org/abs/2004.02707"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Fine-Grained-R2R"><button id="touch">FGR2R Data</button></a>

------

Research
======
<a target="_blank" href="https://arxiv.org/abs/2307.15644">**Scaling Data Generation in Vision-and-Language Navigation**<a/><br>
Zun Wang, Jialu Li, **Yicong Hong**, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao<br>
<em>International Conference on Computer Vision (ICCV), 2023<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2307.15644"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/wz0919/ScaleVLN"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2307.12335">**Learning Navigational Visual Representations with Semantic Map Supervision**<a/><br>
**Yicong Hong**, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, Hao Tan<br>
<em>International Conference on Computer Vision (ICCV), 2023<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2307.12335"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Ego2Map-NaViT"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2203.02764">**Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation**<a/><br>
**Yicong Hong**, Zun Wang, Qi Wu, Stephen Gould<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2022<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2203.02764"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Discrete-Continuous-VLN"><button id="touch">Code</button></a>
  
<a target="_blank" href="https://arxiv.org/abs/2011.13922">**A Recurrent Vision-and-Language BERT for Navigation**<a/><br>
**Yicong Hong**, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2021<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2011.13922"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Recurrent-VLN-BERT"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2010.09304">**Language and Visual Entity Relationship Graph for Agent Navigation**<a/><br>
**Yicong Hong**, Cristian Rodriguez-Opazo, Yuankai Qi, Qi Wu, Stephen Gould<br>
<em>Conference on Neural Information Processing Systems (NeurIPS), 2020<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2010.09304"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Entity-Graph-VLN"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2004.02707">**Sub-Instruction Aware Vision-and-Language Navigation**<a/><br>
**Yicong Hong**, Cristian Rodriguez-Opazo, Qi Wu, Stephen Gould<br>
<em>Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2004.02707"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Fine-Grained-R2R"><button id="touch">FGR2R Data</button></a>

<a target="_blank" href="https://arxiv.org/abs/2206.11610">**1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)**<a/>
<br>Dong An, Zun Wang, Yangguang Li, Yi Wang, **Yicong Hong**, Yan Huang, Liang Wang, Jing Shao<br>
<em>Room-Across-Room (RxR) Habitat Challenge (CVPR Embodied AI Workshop), 2022<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2206.11610"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Discrete-Continuous-VLN"><button id="touch">Code</button></a>
  
<a target="_blank" href="https://arxiv.org/abs/2104.04167">**HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation**<a/><br>
Yanyuan Qiao, Yuankai Qi, **Yicong Hong**, Zheng Yu, Peng Wang, Qi Wu<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2022<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2203.11591"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YanyuanQiao/HOP-VLN"><button id="touch">Code</button></a>
  
<a target="_blank" href="https://arxiv.org/abs/2104.04167">**The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation**<a/><br>
Yuankai Qi, Zizheng Pan, **Yicong Hong**, Ming-Hsuan Yang, Anton van den Hengel, Qi Wu<br>
<em>International Conference on Computer Vision Systems (ICCV), 2021<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2104.04167"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YuankaiQi/ORIST"><button id="touch">Code</button></a>

